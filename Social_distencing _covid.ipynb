{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Destencing Detector using Python | Deep learning | OpenCV\n",
    "\n",
    "\n",
    "   In this artical, we will learn about Covid-19 disease\n",
    "    - how to protect yourself by covid-19.\n",
    "    - how does corona infection spread.\n",
    "    \n",
    "   Coronavirus disease(Novel Covid-19) is an infection or a large group of viruses the consist\n",
    "   of a core of genetic material surrounded by an envelope with protein spikes, this gives it the\n",
    "   the appearance of the crown. a crown is Latin is called corona. 2019 novel coronavirus was first \n",
    "   identified in the city of Wuhan in chaina it initially occurred in a group of people \n",
    "   with pneumonia.\n",
    "    \n",
    "   TRANSMISSION:\n",
    "           \n",
    "   In general respiratory viruses  are usually transmitted through droplets created when an\n",
    "   infected person coughs or sneezes or through something that has been contaminated with the\n",
    "   virus people most at risk of infection from the novel coronavirus are those in close contact\n",
    "   with animals.\n",
    "    \n",
    "    \n",
    "    \n",
    "   ## YOLO (you only look once) in live video\n",
    "   Earlier Darknet framework was built for YOLO algorithm but it only worked with Linux.\n",
    "   then the dark flow framework came into play which along with TensorFlow worked on MAC, Linux,\n",
    "   and Windows. After Opencv 3.2 version, Opencv had its own framework which was compatible\n",
    "   with YOLO. This example is using Opencv with Python. Here we are using a pre-trained model\n",
    "   of the coco dataset which has 80 classes(objects). YOLO has four versions so far. We will \n",
    "   be demonstrating the third version here.\n",
    "  \n",
    "    \n",
    "  \n",
    "  ## REQUIREMENT\n",
    "  \n",
    "        Setup yolov3 Prerequistics\n",
    "      - yolov3.cfg (configration files\n",
    "      - yolov3.weights (treained model to detect objects)\n",
    "      - coco.names (Dataset of 80 objects)\n",
    "      \n",
    "   you can find these three files on the internet yolo official website simply download it.\n",
    "      \n",
    "   # Coding and Implimentation\n",
    "      \n",
    "   First, we will import all the required libraries. There are four sections that we will need to\n",
    "   code in order to get our social distancing analyzer app.\n",
    "      \n",
    "   Then, load yolov3. weight file and load yolov3.cfg (configuration file) in the network that is \n",
    "   \"net\" \n",
    "   here \"dnn\" stands for the deep neural network.\n",
    "      \n",
    "   We then define an empty list \"classes\", in which we want to put \"person\" will be read from the\n",
    "   coco. name file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "# Load Yolo\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  From the net, we are loading the layer names.\n",
    "  In output_layers we get detection of every object.\n",
    "  \n",
    "  Then assigning random colors to each and every class is done. if we have 80 objects in our classes \n",
    "  than 80 random colors have been assigned. The '3' is the channel (for RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.78148262e+01, 1.11711143e+02, 8.95348559e+01],\n",
       "       [4.11589455e+01, 3.10054491e+01, 2.53640310e+02],\n",
       "       [5.89014173e+01, 5.77650352e+01, 1.44799304e+02],\n",
       "       [1.85157028e+02, 1.32016589e+02, 2.37480510e+02],\n",
       "       [1.31799904e+02, 6.43473306e+01, 2.14883749e+02],\n",
       "       [1.15956949e+02, 4.33950269e+01, 9.44859341e+01],\n",
       "       [1.95256039e+02, 4.59801874e+00, 1.45533407e+02],\n",
       "       [9.56532120e+01, 9.80329937e+01, 1.83428965e+02],\n",
       "       [1.01490713e+02, 7.16383000e+01, 1.72035521e+02],\n",
       "       [3.90126649e+01, 2.28397302e+02, 1.59873108e+02],\n",
       "       [7.07766179e+00, 4.99149392e+01, 1.21064590e+02],\n",
       "       [6.85439602e+01, 2.05015153e+02, 2.54757937e+02],\n",
       "       [1.04779265e+02, 5.96938896e+01, 2.49383557e+02],\n",
       "       [7.43875936e+01, 2.20704574e+02, 1.54811698e+02],\n",
       "       [6.85763816e+01, 1.88246101e+02, 2.09276007e+02],\n",
       "       [1.92923231e+02, 1.54681069e+02, 1.45250342e+02],\n",
       "       [1.40833326e+02, 1.94604886e+02, 2.23167039e+02],\n",
       "       [1.33016718e+02, 1.03073066e+02, 2.75943944e+01],\n",
       "       [3.13126382e+01, 5.60969957e+01, 9.96425527e+01],\n",
       "       [2.32467093e+02, 2.01267797e+02, 8.74304535e+01],\n",
       "       [1.50849996e+02, 3.98900817e+01, 1.19023873e+02],\n",
       "       [2.01194640e+02, 1.12973691e+01, 4.69018353e+01],\n",
       "       [1.65845908e+02, 1.46668596e+02, 1.05239507e+02],\n",
       "       [2.97973138e+01, 2.20936469e+02, 1.69071200e+02],\n",
       "       [1.13173087e+02, 1.68205408e+02, 1.37686727e+02],\n",
       "       [1.59277231e+01, 7.54692915e+01, 1.12722024e+02],\n",
       "       [7.80647408e+01, 1.42974217e+02, 1.79187650e+01],\n",
       "       [1.43263232e+02, 1.25052213e+02, 1.80862701e+02],\n",
       "       [2.85529828e+00, 1.15484301e+02, 2.16979103e+02],\n",
       "       [1.38163154e+02, 1.39118125e+02, 2.33094840e+02],\n",
       "       [4.44987422e+01, 1.73210457e+02, 1.23389178e+02],\n",
       "       [3.52315270e+01, 1.54756815e+02, 1.33284509e+02],\n",
       "       [2.39698601e+02, 1.47187393e+02, 1.06284964e+02],\n",
       "       [1.15715102e+02, 2.14651525e+02, 2.44903737e+02],\n",
       "       [4.79140588e+01, 1.96771742e+02, 2.15634261e+02],\n",
       "       [8.58198322e+01, 1.18825970e+02, 1.00368892e+02],\n",
       "       [5.21238972e+01, 1.41748336e+02, 1.93360570e+01],\n",
       "       [2.42262404e+02, 1.57125746e+02, 3.37093257e+01],\n",
       "       [4.56477565e+01, 2.12140514e+02, 1.16292957e+01],\n",
       "       [2.51509907e+01, 2.30747844e+02, 4.81938036e+01],\n",
       "       [1.33197846e+02, 6.13444365e+01, 2.00752082e+02],\n",
       "       [6.09887112e+01, 1.99362633e+02, 7.65954085e-01],\n",
       "       [1.12430362e+02, 1.19787900e+02, 1.51607544e+02],\n",
       "       [9.93887959e+01, 1.64850310e+02, 4.35123108e+01],\n",
       "       [1.44730589e+02, 1.66314627e+02, 8.69589422e+01],\n",
       "       [1.86261739e+02, 1.93282837e+02, 2.08691304e+02],\n",
       "       [1.44313682e+02, 2.16632358e+02, 2.84753080e+01],\n",
       "       [2.21653118e+02, 2.33418362e+01, 2.94037910e+01],\n",
       "       [1.74825895e+02, 2.23333454e+02, 1.72050317e+02],\n",
       "       [1.43384126e+02, 1.22523945e+02, 1.52344774e+02],\n",
       "       [1.83905423e+02, 2.46401628e+02, 2.10361175e+02],\n",
       "       [2.33133908e+01, 6.54767327e+01, 2.35988698e+02],\n",
       "       [8.31888749e+01, 1.46137866e+02, 2.18716067e+02],\n",
       "       [1.87972903e+02, 1.04942363e+02, 2.19510316e+02],\n",
       "       [1.33288093e+02, 2.35337550e+02, 5.84739381e+01],\n",
       "       [9.38434167e+00, 1.59025746e+02, 1.08389111e+02],\n",
       "       [1.77842639e+02, 1.53923719e+02, 2.42874480e+02],\n",
       "       [4.89579297e+01, 1.96399942e+02, 1.56274676e+02],\n",
       "       [1.99894164e+02, 2.06551877e+02, 2.53780307e+02],\n",
       "       [2.40279604e+02, 1.80480024e+02, 9.78101341e+01],\n",
       "       [1.05452652e+02, 1.94147621e+02, 1.21602007e+02],\n",
       "       [1.62724480e+02, 6.57887636e+01, 6.85160736e+01],\n",
       "       [1.98707427e+01, 1.09793335e+02, 7.73836835e+01],\n",
       "       [6.60533662e+01, 1.19801628e+02, 1.30639238e+02],\n",
       "       [2.16028052e+02, 1.86948286e+02, 2.09336404e+02],\n",
       "       [8.32820471e+01, 1.19983555e+02, 2.30729493e+02],\n",
       "       [6.87648523e+01, 2.11653651e+02, 9.47662524e+01],\n",
       "       [1.08564973e+02, 5.53747510e+01, 4.12802376e+01],\n",
       "       [2.26110058e+02, 1.49709581e+02, 1.64220831e+02],\n",
       "       [9.07938296e+01, 1.51168878e+01, 1.88266076e+02],\n",
       "       [7.61956385e+01, 2.39881398e+02, 1.31553556e+02],\n",
       "       [1.97655397e+02, 5.56769014e+01, 5.48319126e+01],\n",
       "       [2.12495759e+02, 2.15399006e-01, 2.20931352e+02],\n",
       "       [8.46485587e+01, 9.81806571e+01, 1.38662029e+02],\n",
       "       [6.84416405e+01, 2.07346011e+02, 1.12746768e+02],\n",
       "       [1.15873998e+02, 2.24893007e+02, 3.30014757e+01],\n",
       "       [1.24060125e+02, 1.30528309e+02, 2.12885010e+02],\n",
       "       [8.53316334e+01, 1.68642646e+02, 1.83304256e+02],\n",
       "       [3.61760830e+01, 2.40290010e+02, 6.36156634e+01],\n",
       "       [1.17230189e+02, 1.26148460e+02, 1.46126158e+02]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "np.random.uniform(0, 255, size=(len(classes),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Now, we will read an image in which objects need to be detected using imread() function from\n",
    "   OpenCV\n",
    "\n",
    "   Since the image we are using is too large hence, we need to resize the image.\n",
    "   We use cv2.resize function for it. The first attribute passed is the image name.\n",
    "   The 'None' refers that we are not specifying any particular size of the image.\n",
    "   The fx and fy having 0.4 states that 40% should be the height and width of the image. \n",
    "   The height, width, and channel needs to have the same shape as that of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('video.mp4')\n",
    "#cap = cv2.VideoCapture(0)\n",
    "#FourCC code is passed as\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "output = cv2.VideoWriter('output4.mp4',fourcc, 20.0, (img.shape[1], img.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Select which camera to use for the live video you should use 0 or for additional camera use 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap = cv2.VideoCapture('http://192.168.43.1:8080//video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   If you want to use CCTV or IP Webcam put the URL under single codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibration needed for each video\n",
    "\n",
    "def calibrated_dist(p1, p2):\n",
    "    return ((p1[0] - p2[0]) ** 2 + 550 / ((p1[1] + p2[1]) / 2) * (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "\n",
    "def isclose(p1, p2):\n",
    "    c_d = calibrated_dist(p1, p2)\n",
    "    calib = (p1[1] + p2[1]) / 2\n",
    "    if 0 < c_d < 0.15 * calib:\n",
    "        return 1\n",
    "    elif 0 < c_d < 0.2 * calib:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "height,width=(None,None)\n",
    "\n",
    "q=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Take two points for Euclidean Distance by using the \"calibrated_dist\" function and return it.\n",
    "\n",
    "   then populate the function that calculates the Euclidean distance between two points. \n",
    "   The distance equals to the root of the sum of the squared point and we returned our distance.\n",
    "    \n",
    "   if p1 and p2 are too close then return 1 if the distance is medium then return 2 else 0.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(cap.isOpened()):\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "\n",
    "    ret, img = cap.read()\n",
    "       \n",
    "    print(ret)\n",
    "    \n",
    "    if not ret:\n",
    "    \n",
    "        break\n",
    "    \n",
    "    if width is None or height is None:\n",
    "        \n",
    "        height,width=img.shape[:2]\n",
    "        q=width\n",
    "       \n",
    "        \n",
    "\n",
    "    #height, width, channels = img.shape\n",
    "    \n",
    "    img =img[0:height, 0:q]\n",
    "    height,width=img.shape[:2]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Detecting objects 0.00392\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(img,0.00392, (416, 416), (0,0,0), True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    end=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We can't give the image directly to the algorithm. We first have to convert it into the blob.\n",
    "Blob extracts the features from the image. (416,416) is the standard size. \n",
    "\n",
    "Then we pass the blob image to the algorithm.\n",
    "\n",
    "Then, finally, we feed the output layer to 'outs' for the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     # Showing informations on the screen\n",
    "\n",
    "    class_ids = []\n",
    "\n",
    "    confidences = []\n",
    "\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "\n",
    "        for detection in out:\n",
    "\n",
    "            scores = detection[5:]\n",
    "\n",
    "            class_id = np.argmax(scores)\n",
    "\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            #0.5 is the threshold for confidence\n",
    "            if confidence > 0.5:\n",
    "\n",
    "                # Object detected\n",
    "                #Purpose : Converts center coordinates to rectangle coordinates\n",
    "                # x, y = midpoint of box\n",
    "\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                \n",
    "                # w, h = width, height of the box\n",
    "                w = int(detection[2] * width)\n",
    "\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "                class_ids.append(class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   We create three empty lists i.e., class_ids, confidences, and boxes.\n",
    "\n",
    "   Firstly scores will be calculated.\n",
    "   The class_ids will contain all the class (object) id of the objects which will be detected.\n",
    "   The class_ids will be decided as per the maximum argument.\n",
    "\n",
    "   The confidence will store the score of the class_id. \n",
    "\n",
    "   If confidence>0.5 (50%), then the object will be detected. Then we will find the center\n",
    "   points as well as (x,y),(w,h) which are used to determine top left and bottom right\n",
    "   coordinates of the rectangle respectively.\n",
    "\n",
    "   Only if the confidence is greater than the confidence threshold, then only class_id,\n",
    "   confidence, and scores will be appended in the list. The scores contain (x,y,w,h) of the \n",
    "   rectangle.\n",
    "\n",
    "   Here 0.5 is the threshold for confidence, but you can use change according to your project \n",
    "   requirement. High scoring regions of the image are considered for detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.5)\n",
    "\n",
    "    #print(indexes)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    if len(indexes)>0:\n",
    "        \n",
    "        status=list()\n",
    "        \n",
    "        idf = indexes.flatten()\n",
    "        \n",
    "        close_pair = list()\n",
    "        \n",
    "        s_close_pair = list()\n",
    "        \n",
    "        center = list()\n",
    "        \n",
    "        dist = list()\n",
    "        \n",
    "        for i in idf:\n",
    "            \n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            \n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            \n",
    "            center.append([int(x + w / 2), int(y + h / 2)])\n",
    "            \n",
    "            status.append(0)\n",
    "            \n",
    "        for i in range(len(center)):\n",
    "            \n",
    "            for j in range(len(center)):\n",
    "                \n",
    "                #compare the closeness of two values\n",
    "                g=isclose(center[i], center[j])\n",
    "                \n",
    "                if g ==1:\n",
    "                    \n",
    "                    close_pair.append([center[i],center[j]])\n",
    "                    \n",
    "                    status[i] = 1\n",
    "                    \n",
    "                    status[j] = 1\n",
    "                    \n",
    "                elif g == 2:\n",
    "                    \n",
    "                    s_close_pair.append([center[i], center[j]])\n",
    "                    \n",
    "                    if status[i] != 1:\n",
    "                        \n",
    "                        status[i] = 2\n",
    "                        \n",
    "                    if status[j] != 1:\n",
    "                        \n",
    "                        status[j] = 2\n",
    "        total_p = len(center)\n",
    "        \n",
    "        low_risk_p = status.count(2)\n",
    "        \n",
    "        high_risk_p = status.count(1)\n",
    "        \n",
    "        safe_p = status.count(0)\n",
    "        \n",
    "        kk = 0\n",
    "        \n",
    "        for i in idf:\n",
    "            \n",
    "            sub_img = img[10:170, 10:width - 10]\n",
    "            \n",
    "            black_rect = np.ones(sub_img.shape, dtype=np.uint8)*0\n",
    "            \n",
    "            res = cv2.addWeighted(sub_img, 0.77, black_rect,0.23, 1.0)\n",
    "\n",
    "            img[10:170, 10:width - 10] = res\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The Non-max suppression removes multiple boxes around the same object.\n",
    "   From boxes, we extract the x,y,w,h coordinates of the object and label them with their class_ids. \n",
    "\n",
    "   Then random colors generated earlier are assigned to the color variable.\n",
    "\n",
    "   In cv2.rectangle, using the (x,y,w,h), we draw a rectangle around the objects detected.\n",
    "   (x,y) are the top lest coordinates and (w,h) refers to height and width respectively.\n",
    "   Hence, x+w and y+h gives the bottom-right coordinate of the rectangle. 'color' is passed to\n",
    "   cv2.rectangle to pass the assigned unique color of each object. The '2' in the function is \n",
    "   the width of the rectangle.\n",
    "\n",
    "   Finally, we label (assign text) to the object detected and label it with the class_id.\n",
    "   If the object detected is a human, then it would be labeled as 'person'. This is done via\n",
    "   cv2.putText() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding text to image\n",
    "            \n",
    "                      #(image,text,org( X coordinate value, Y coordinate value),font,fontScale,color,thikness)\n",
    "            cv2.putText(img, \"Social Distancing Detection - During COVID19 \", (255, 45),font, 1, (255, 255, 255), 2)\n",
    "            cv2.putText(img, \"GOEDUHUB TECHNOLOGY\", (450, 200),font, 1, (0, 255, 255), 2)\n",
    "            \n",
    "            #image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "            cv2.rectangle(img, (20, 60), (625, 160), (170, 170, 170), 2)\n",
    "            \n",
    "            cv2.putText(img, \"Connecting lines shows closeness among people. \", (45, 80),font, 0.6, (255, 255, 0), 1)\n",
    "            \n",
    "            cv2.putText(img, \"YELLOW: CLOSE\", (45, 110),font, 0.5, (0, 255, 255), 1)\n",
    "            \n",
    "            cv2.putText(img, \"RED: VERY CLOSE\", (45, 130),font, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "            cv2.rectangle(img, (675, 60), (width -20, 160), (170, 170, 170), 2)\n",
    "            \n",
    "            cv2.putText(img, \"Bounding box shows the level of risk to the person.\",(685, 80),font, 0.6, (255, 255, 0), 1)\n",
    "            \n",
    "            \n",
    "            cv2.putText(img, \"DARK RED: HIGH RISK\", (685, 110),font, 0.5, (0, 0, 150), 1)\n",
    "            \n",
    "            cv2.putText(img, \"ORANGE: LOW RISK\", (685, 130),font, 0.5, (0, 120, 255), 1)\n",
    "\n",
    "            cv2.putText(img, \"GREEN: CONGRATULATIONS YOU ARE SAFE\", (685, 150),font, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            tot_str = \"NUMBER OF PEOPLE: \" + str(total_p)\n",
    "            \n",
    "            high_str = \"RED ZONE: \" + str(high_risk_p)\n",
    "            \n",
    "            low_str = \"ORANGE ZONE: \" + str(low_risk_p)\n",
    "            \n",
    "            safe_str = \"GREEN ZONE: \" + str(safe_p)\n",
    "            \n",
    "            #image ROI\n",
    "            sub_img = img[height - 120:height-20, 0:500]\n",
    "            #cv2.imshow(\"sub_img\",sub_img)\n",
    "            \n",
    "            black_rect = np.ones(sub_img.shape, dtype=np.uint8) * 0\n",
    "\n",
    "            res = cv2.addWeighted(sub_img, 0.8, black_rect, 0.2, 1.0)\n",
    "\n",
    "            img[height - 120:height-20, 0:500] = res\n",
    "\n",
    "            cv2.putText(img, tot_str, (10, height - 75),font, 0.6, (255, 255, 255), 1)\n",
    "            \n",
    "            cv2.putText(img, safe_str, (300, height - 75),font, 0.6, (0, 255, 0), 1)\n",
    "            \n",
    "            cv2.putText(img, low_str, (10, height - 50),font, 0.6, (0, 120, 255), 1)\n",
    "            \n",
    "            cv2.putText(img, high_str, (300, height - 50),font, 0.6, (0, 0, 150), 1)\n",
    "\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            \n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            \n",
    "            \n",
    "           #color of the ractangle when is too close \n",
    "            if status[kk] == 1:\n",
    "                \n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 150), 2)\n",
    "\n",
    "            elif status[kk] == 0:\n",
    "                \n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            else:\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 120, 255), 2)\n",
    "\n",
    "            kk += 1\n",
    "        for h in close_pair:\n",
    "            \n",
    "            cv2.line(img, tuple(h[0]), tuple(h[1]), (0, 0, 255), 2)\n",
    "            \n",
    "        for b in s_close_pair:\n",
    "            \n",
    "            cv2.line(img, tuple(b[0]), tuple(b[1]), (0, 255, 255), 2)\n",
    "\n",
    "    \n",
    "    cv2.imshow('image',img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    cv2.waitKey(1)\n",
    "    output.write(img)\n",
    "cap.release()\n",
    "output.release()\n",
    "cv2.destroyAllWindows()\n",
    "# press 'q' to release the window.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   For adding text to images we need:-\n",
    "      -frame in which you want to put the data(text)\n",
    "      -position coordinates of where you want to put it. (that is the bottom-left corner where\n",
    "        data starts).\n",
    "      -font type (check cv2.putText() docs for supported fonts)\n",
    "      -font scale (specifies the size of the font)\n",
    "      -regular things like color, thickness, lineType, etc. for better look,\n",
    "        lineType = cv2.LINE_AA is recommended.\n",
    "         \n",
    "   cv2.rectangle use for drow a rectangle under which we put text data \n",
    "         \n",
    "   On frame using the \"sub_img\" tag draws a sub-image in which the results are displayed.\n",
    "   black_rect use to draw a black rectangle on the top of the screen.\n",
    "         \n",
    "   if two people are too close then rectangle change the color according to status like red\n",
    "   rectangle, yellow and if the person is far away from another then it displays green\n",
    "   rectangle and also draw a line between them which calculate the distance between \n",
    "   two center points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set out for video writer process it frame-by-frame and we want to save that video.\n",
    "For images, it is very simple, just use cv2.imwrite().\n",
    "Here a little more work is required.\n",
    "FourCC code is passed as cv2.VideoWriter_fourcc('M','J','P','G') or \n",
    "cv2.VideoWriter_fourcc(*'MJPG) for MJPG.\n",
    "\n",
    "Finally, we are displaying the output.\n",
    "\n",
    "Using cv2.imshow(), we display image. The first argument is the frame name and the\n",
    "the second argument is the image in which an object is detected.\n",
    "\n",
    "The waitKey() stops the output on the screen. Else, it will be displayed but for a very\n",
    "the short interval which won't be visible.\n",
    "\n",
    "The cv2.destroyAllWindows() then destroy all the open windows. Without it the output\n",
    "will hang and we will have to restart the kernel every time we run the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
